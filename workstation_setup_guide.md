# ì›Œí¬ìŠ¤í…Œì´ì…˜ AI ì„œë²„ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•˜ë“œì›¨ì–´ ìµœì†Œ ì‚¬ì–‘
- [ ] GPU: NVIDIA RTX 3060 (12GB) ì´ìƒ
- [ ] RAM: 32GB ì´ìƒ
- [ ] ì €ì¥ê³µê°„: 100GB ì´ìƒ ì—¬ìœ 
- [ ] ë„¤íŠ¸ì›Œí¬: ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²°

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- [ ] Ubuntu 20.04+ ë˜ëŠ” Windows 10/11
- [ ] NVIDIA ë“œë¼ì´ë²„ (525.x ì´ìƒ)
- [ ] CUDA 11.8 ë˜ëŠ” 12.1
- [ ] Python 3.8-3.10
- [ ] Docker (ì„ íƒì‚¬í•­)

## ğŸš€ Step 1: ì›Œí¬ìŠ¤í…Œì´ì…˜ í™˜ê²½ ì„¤ì •

### 1.1 GPU í™˜ê²½ í™•ì¸
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# PyTorch GPU ì§€ì› í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
```

### 1.2 Python í™˜ê²½ êµ¬ì„±
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv naviyam_gpu_env
source naviyam_gpu_env/bin/activate  # Linux/Mac
# ë˜ëŠ”
naviyam_gpu_env\Scripts\activate  # Windows

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install fastapi uvicorn transformers accelerate bitsandbytes
```

## ğŸ–¥ï¸ Step 2: AI ì„œë²„ ë¡œì»¬ ì‹¤í–‰

### 2.1 ë‚˜ë¹„ì–Œ ë°±ì—”ë“œ ë³µì‚¬
```bash
# ì›Œí¬ìŠ¤í…Œì´ì…˜ì— í”„ë¡œì íŠ¸ ë³µì‚¬
scp -r public_projects/naviyam_backend_runnable_20250806_112316 workstation:/home/user/
```

### 2.2 GPU ìµœì í™” ì„œë²„ íŒŒì¼ ìƒì„±
```python
# gpu_server.py
import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Naviyam GPU Server")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    user_id: str = "default"
    session_id: str = "default"

class ChatResponse(BaseModel):
    response: str
    recommendations: list = []

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
model = None
tokenizer = None

@app.on_event("startup")
async def load_model():
    global model, tokenizer
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"GPU ë©”ëª¨ë¦¬: {gpu_mem:.1f}GB")
    
    # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
    model_name = "maywell/EXAONE-3.0-7.8B-Instruct"  # ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸
    
    from transformers import BitsAndBytesConfig
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
        )
        logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # í´ë°±: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
        model_name = "microsoft/DialoGPT-medium"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name).cuda()

@app.get("/")
def root():
    return {"status": "Naviyam GPU Server Running", "gpu": torch.cuda.is_available()}

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "model_loaded": model is not None
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer.encode(request.message, return_tensors="pt").cuda()
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return ChatResponse(
            response=response,
            recommendations=[]
        )
    
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2.3 ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```bash
# run_gpu_server.sh
#!/bin/bash

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
nvidia-smi
echo "GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘..."
sudo fuser -k /dev/nvidia*

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# ì„œë²„ ì‹¤í–‰
source naviyam_gpu_env/bin/activate
python gpu_server.py
```

## ğŸŒ Step 3: ì™¸ë¶€ ì ‘ì† ì„¤ì •

### ì˜µì…˜ A: ngrok (ê°€ì¥ ê°„ë‹¨)
```bash
# ngrok ì„¤ì¹˜
curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
sudo apt update && sudo apt install ngrok

# ê³„ì • ì„¤ì • (ë¬´ë£Œ ê°€ì… í›„)
ngrok config add-authtoken YOUR_AUTH_TOKEN

# í„°ë„ ì‹¤í–‰
ngrok http 8000

# ì¶œë ¥ ì˜ˆì‹œ:
# Forwarding https://abc123.ngrok.io -> http://localhost:8000
```

### ì˜µì…˜ B: Cloudflare Tunnel (ë” ì•ˆì •ì )
```bash
# cloudflared ì„¤ì¹˜
wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
sudo dpkg -i cloudflared-linux-amd64.deb

# í„°ë„ ìƒì„±
cloudflared tunnel create naviyam-gpu

# ì„¤ì • íŒŒì¼ ìƒì„±
cat > ~/.cloudflared/config.yml << EOF
tunnel: naviyam-gpu
credentials-file: /home/user/.cloudflared/[TUNNEL_ID].json

ingress:
  - hostname: naviyam-gpu.yourdomain.com
    service: http://localhost:8000
  - service: http_status:404
EOF

# í„°ë„ ì‹¤í–‰
cloudflared tunnel run naviyam-gpu
```

### ì˜µì…˜ C: í¬íŠ¸ í¬ì›Œë”© (ê³ ì • IP í•„ìš”)
```bash
# ë¼ìš°í„° ì„¤ì •ì—ì„œ í¬íŠ¸ í¬ì›Œë”©
# ì™¸ë¶€ í¬íŠ¸ 8000 -> ë‚´ë¶€ IP:8000

# ë°©í™”ë²½ ì„¤ì •
sudo ufw allow 8000/tcp

# DDNS ì„¤ì • (ë™ì  IPì¸ ê²½ìš°)
# DuckDNS, No-IP ë“± ì‚¬ìš©
```

## ğŸ”— Step 4: Cloud Runê³¼ ì—°ë™

### 4.1 Cloud Run ì„œë¹„ìŠ¤ ìˆ˜ì •
```python
# public_projects/naviyam_backend_runnable_20250806_112316/api/server.py ìˆ˜ì •

import os
import requests
from fastapi import FastAPI, HTTPException

# ì›Œí¬ìŠ¤í…Œì´ì…˜ GPU ì„œë²„ URL
GPU_SERVER_URL = os.getenv("GPU_SERVER_URL", "https://your-tunnel.ngrok.io")

@app.post("/chat")
async def chat(request: ChatRequest):
    try:
        # GPU ì„œë²„ë¡œ ì „ë‹¬
        response = requests.post(
            f"{GPU_SERVER_URL}/chat",
            json=request.dict(),
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            # í´ë°±: ê°„ë‹¨í•œ ì‘ë‹µ
            return {
                "response": "GPU ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ê¸°ë³¸ ì‘ë‹µì…ë‹ˆë‹¤.",
                "recommendations": []
            }
    
    except requests.exceptions.RequestException as e:
        logger.error(f"GPU server error: {e}")
        # í´ë°± ì²˜ë¦¬
        return {
            "response": "í˜„ì¬ ì„œë¹„ìŠ¤ê°€ ì›í™œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "recommendations": []
        }
```

### 4.2 Cloud Run ì¬ë°°í¬
```bash
# í™˜ê²½ ë³€ìˆ˜ì™€ í•¨ê»˜ ë°°í¬
gcloud run deploy nabiyam-chatbot-web \
  --source . \
  --set-env-vars GPU_SERVER_URL=https://your-tunnel.ngrok.io \
  --region asia-northeast3 \
  --project rational-autumn-467006-e2
```

## ğŸ”’ Step 5: ë³´ì•ˆ ë° ëª¨ë‹ˆí„°ë§

### 5.1 API í‚¤ ì¸ì¦ ì¶”ê°€
```python
# gpu_server.pyì— ì¶”ê°€
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi import Depends

security = HTTPBearer()

API_KEY = os.getenv("API_KEY", "your-secret-key")

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    if credentials.credentials != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return credentials.credentials

@app.post("/chat", dependencies=[Depends(verify_token)])
async def chat(request: ChatRequest):
    # ê¸°ì¡´ ì½”ë“œ
```

### 5.2 ëª¨ë‹ˆí„°ë§ ì„¤ì •
```python
# monitoring.py
import psutil
import GPUtil
from datetime import datetime

def log_system_status():
    # CPU ì‚¬ìš©ë¥ 
    cpu_percent = psutil.cpu_percent(interval=1)
    
    # RAM ì‚¬ìš©ë¥ 
    ram = psutil.virtual_memory()
    
    # GPU ìƒíƒœ
    gpus = GPUtil.getGPUs()
    
    status = {
        "timestamp": datetime.now().isoformat(),
        "cpu_percent": cpu_percent,
        "ram_percent": ram.percent,
        "gpu_memory": gpus[0].memoryUsed if gpus else 0,
        "gpu_util": gpus[0].load * 100 if gpus else 0
    }
    
    # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    with open("system_monitor.log", "a") as f:
        f.write(f"{status}\n")
    
    return status
```

### 5.3 ìë™ ì¬ì‹œì‘ ì„¤ì •
```bash
# systemd ì„œë¹„ìŠ¤ ìƒì„±
sudo nano /etc/systemd/system/naviyam-gpu.service

[Unit]
Description=Naviyam GPU Server
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/home/user/naviyam
ExecStart=/home/user/naviyam_gpu_env/bin/python gpu_server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target

# ì„œë¹„ìŠ¤ í™œì„±í™”
sudo systemctl enable naviyam-gpu
sudo systemctl start naviyam-gpu
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ëª¨ë¸ ìºì‹±
```python
# ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€
from functools import lru_cache

@lru_cache(maxsize=1)
def get_model():
    return load_model()
```

### 2. ë°°ì¹˜ ì²˜ë¦¬
```python
# ì—¬ëŸ¬ ìš”ì²­ì„ ëª¨ì•„ì„œ ì²˜ë¦¬
from asyncio import Queue
batch_queue = Queue(maxsize=10)

async def batch_inference():
    batch = []
    while len(batch) < 5:
        request = await batch_queue.get()
        batch.append(request)
    
    # ë°°ì¹˜ ì²˜ë¦¬
    results = model.generate_batch(batch)
    return results
```

### 3. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
import gc
torch.cuda.empty_cache()
gc.collect()
```

## ğŸ¯ ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GPU ì„œë²„ ì •ìƒ ì‹¤í–‰ í™•ì¸
- [ ] ì™¸ë¶€ ì ‘ì† URL ìƒì„± ì™„ë£Œ
- [ ] Cloud Run í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] API í‚¤ ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ìë™ ì¬ì‹œì‘ ì„¤ì •
- [ ] í…ŒìŠ¤íŠ¸ ì™„ë£Œ

## ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
model_name = "microsoft/DialoGPT-medium"  # 345M

# ë˜ëŠ” 8bit ì–‘ìí™”
load_in_8bit=True
```

### ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì •
```bash
# ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
max_retries = 3
retry_delay = 5
```

### ì „ì› ê´€ë¦¬
```bash
# GPU ì ˆì „ ëª¨ë“œ ë¹„í™œì„±í™”
sudo nvidia-smi -pm 1
```