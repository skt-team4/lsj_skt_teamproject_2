#!/usr/bin/env python3
"""
A.X-3.1-Light 4bit ì–‘ìí™” ì„œë²„ - ì„¸ì…˜ ì§€ì› ë²„ì „
ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ì—¬ ì—°ì†ì ì¸ ëŒ€í™” ê°€ëŠ¥
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import datetime
import uvicorn
import logging
from collections import defaultdict
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="A.X-3.1-Light 4bit Server with Session",
    description="ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ì§€ì› - Swagger UIì—ì„œ ì—°ì† ëŒ€í™” í…ŒìŠ¤íŠ¸ ê°€ëŠ¥",
    version="1.0.0"
)

# CORS ì„¤ì • (ì›¹ UI ì ‘ê·¼ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
conversation_history = defaultdict(list)  # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    max_history: Optional[int] = 5  # ìœ ì§€í•  ëŒ€í™” í„´ ìˆ˜

class SessionRequest(BaseModel):
    session_id: str

class ChatResponse(BaseModel):
    response: str
    session_id: str
    turn_count: int
    gpu_status: str
    quantization: str
    timestamp: str

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer
    
    logger.info("ğŸš€ A.X-3.1-Light ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    
    try:
        # 4bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model_name = "skt/A.X-3.1-Light"
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
        logger.info("ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"âœ… GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB")
        
        logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! 4bit ì–‘ìí™” í™œì„±í™”ë¨")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/", tags=["Info"])
def root():
    """ì„œë¹„ìŠ¤ ì •ë³´"""
    return {
        "service": "A.X-3.1-Light 4bit Server with Session",
        "status": "running",
        "model_loaded": model is not None,
        "active_sessions": len(conversation_history),
        "endpoints": {
            "/docs": "Swagger UI - ëŒ€í™” í…ŒìŠ¤íŠ¸",
            "/chat": "ëŒ€í™” API (ì„¸ì…˜ ì§€ì›)",
            "/session/new": "ìƒˆ ì„¸ì…˜ ìƒì„±",
            "/session/history": "ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ",
            "/session/clear": "ì„¸ì…˜ ì´ˆê¸°í™”"
        }
    }

@app.get("/health", tags=["Info"])
def health():
    """í—¬ìŠ¤ì²´í¬"""
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "device_name": torch.cuda.get_device_name(0),
            "memory_allocated": f"{torch.cuda.memory_allocated(0) / 1e9:.2f} GB",
            "memory_reserved": f"{torch.cuda.memory_reserved(0) / 1e9:.2f} GB",
            "memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB",
            "cuda_version": torch.version.cuda
        }
    else:
        gpu_info = {"available": False}
    
    return {
        "status": "healthy",
        "message": "4bit ì–‘ìí™” ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "gpu": gpu_info,
        "pytorch_version": torch.__version__,
        "model_loaded": model is not None,
        "quantization": "4bit (nf4)",
        "active_sessions": len(conversation_history)
    }

@app.post("/session/new", tags=["Session"], response_model=dict)
async def create_session():
    """ìƒˆ ì„¸ì…˜ ìƒì„±"""
    session_id = str(uuid.uuid4())
    conversation_history[session_id] = []
    return {
        "session_id": session_id,
        "message": "ìƒˆ ì„¸ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ session_idë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì† ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "created_at": datetime.datetime.now().isoformat()
    }

@app.post("/session/history", tags=["Session"])
async def get_history(request: SessionRequest):
    """ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    if request.session_id not in conversation_history:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    history = conversation_history[request.session_id]
    return {
        "session_id": request.session_id,
        "turn_count": len(history),
        "history": history
    }

@app.post("/session/clear", tags=["Session"])
async def clear_session(request: SessionRequest):
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    if request.session_id in conversation_history:
        conversation_history[request.session_id] = []
        return {"message": "ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤", "session_id": request.session_id}
    else:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.post("/chat", tags=["Chat"], response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ëŒ€í™” API - ì„¸ì…˜ì„ í†µí•œ ì—°ì† ëŒ€í™” ì§€ì›
    
    ì‚¬ìš© ë°©ë²•:
    1. /session/newë¡œ session_id ìƒì„±
    2. session_idë¥¼ í¬í•¨í•˜ì—¬ ëŒ€í™” ìš”ì²­
    3. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ìë™ìœ¼ë¡œ ìœ ì§€ë©ë‹ˆë‹¤
    """
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ì„¸ì…˜ ID ì²˜ë¦¬
        if not request.session_id:
            request.session_id = str(uuid.uuid4())
        
        # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        history = conversation_history[request.session_id]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)
        prompt_parts = ["ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ëŒ€í™”ì…ë‹ˆë‹¤.\n"]
        
        # ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        for turn in history[-request.max_history:]:
            prompt_parts.append(f"ì‚¬ìš©ì: {turn['user']}")
            prompt_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {turn['assistant']}")
        
        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        prompt_parts.append(f"ì‚¬ìš©ì: {request.message}")
        prompt_parts.append("ì–´ì‹œìŠ¤í„´íŠ¸:")
        
        prompt = "\n".join(prompt_parts)
        
        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        )
        
        # GPUë¡œ ì´ë™
        input_ids = inputs["input_ids"].to(model.device)
        attention_mask = inputs["attention_mask"].to(model.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ì‘ë‹µ ì¶”ì¶œ
        if "ì–´ì‹œìŠ¤í„´íŠ¸:" in full_response:
            response = full_response.split("ì–´ì‹œìŠ¤í„´íŠ¸:")[-1].strip()
        else:
            response = full_response[len(prompt):].strip()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        history.append({
            "user": request.message,
            "assistant": response,
            "timestamp": datetime.datetime.now().isoformat()
        })
        
        # GPU ìƒíƒœ
        gpu_status = "4bit ì–‘ìí™” GPU ëª¨ë“œ"
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_status = f"4bit GPU ({allocated:.1f}GB ì‚¬ìš©)"
        
        return ChatResponse(
            response=response,
            session_id=request.session_id,
            turn_count=len(history),
            gpu_status=gpu_status,
            quantization="4bit",
            timestamp=datetime.datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/session/list", tags=["Session"])
async def list_sessions():
    """í™œì„± ì„¸ì…˜ ëª©ë¡"""
    sessions = []
    for session_id, history in conversation_history.items():
        sessions.append({
            "session_id": session_id,
            "turn_count": len(history),
            "last_activity": history[-1]["timestamp"] if history else None
        })
    return {"active_sessions": len(sessions), "sessions": sessions}

if __name__ == "__main__":
    print("ğŸš€ A.X-3.1-Light 4bit ì„¸ì…˜ ì„œë²„ ì‹œì‘...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
    print("\nğŸ“š Swagger UI: http://localhost:8000/docs")
    print("ğŸ’¡ /session/newë¡œ ì„¸ì…˜ì„ ìƒì„±í•œ í›„ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")
    uvicorn.run(app, host="0.0.0.0", port=8000)