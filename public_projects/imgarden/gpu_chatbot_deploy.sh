#!/bin/bash

# GPU ì„œë²„ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
echo "ðŸš€ GPU ì±—ë´‡ ë°°í¬ ì‹œìž‘..."

# 1. Python í™˜ê²½ ì„¤ì •
sudo apt-get update
sudo apt-get install -y python3-pip python3-venv git

# 2. ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv /home/isangjae/chatbot_env
source /home/isangjae/chatbot_env/bin/activate

# 3. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install fastapi uvicorn torch transformers accelerate

# 4. ê°„ë‹¨í•œ GPU ì±—ë´‡ ìƒì„±
cat > /home/isangjae/gpu_chatbot.py << 'PYTHON_CODE'
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import datetime
import os

app = FastAPI(title="GPU ì±—ë´‡ ì„œë²„")

class ChatRequest(BaseModel):
    message: str
    user_id: str = "guest"

@app.get("/")
def root():
    return {
        "service": "ë‚˜ë¹„ì–Œ GPU ì±—ë´‡",
        "status": "running",
        "server": "Google Cloud GPU Instance"
    }

@app.get("/health")
def health():
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "device_count": torch.cuda.device_count(),
            "device_name": torch.cuda.get_device_name(0),
            "memory_allocated": f"{torch.cuda.memory_allocated(0) / 1e9:.2f} GB",
            "memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB"
        }
    else:
        gpu_info = {"available": False, "message": "GPU not detected"}
    
    return {
        "status": "healthy",
        "message": "GPU ì„œë²„ê°€ ì •ìƒ ìž‘ë™ ì¤‘ìž…ë‹ˆë‹¤!",
        "timestamp": datetime.datetime.now().isoformat(),
        "gpu": gpu_info,
        "pytorch_version": torch.__version__
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    message = request.message.lower()
    
    # GPU í…ŒìŠ¤íŠ¸ ì—°ì‚°
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # ê°„ë‹¨í•œ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ GPU ì‚¬ìš© í…ŒìŠ¤íŠ¸
        test_tensor = torch.randn(1000, 1000).to(device)
        result = torch.matmul(test_tensor, test_tensor)
        gpu_status = "GPU ì‚¬ìš© ì¤‘ (NVIDIA Tesla T4)"
    else:
        gpu_status = "CPU ëª¨ë“œ"
    
    # ê°„ë‹¨í•œ ì‘ë‹µ ë¡œì§
    responses = {
        "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! GPU ì„œë²„ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ë‚˜ë¹„ì–Œ ì±—ë´‡ìž…ë‹ˆë‹¤! ðŸš€",
        "ì ì‹¬": "ì˜¤ëŠ˜ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œ ì–´ë– ì„¸ìš”? GPUê°€ ë¹ ë¥´ê²Œ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤!",
        "í•œì‹": "ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ëœìž¥ì°Œê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤! GPU íŒŒì›Œë¡œ ì„ íƒí–ˆì–´ìš”!",
        "gpu": f"í˜„ìž¬ {gpu_status}ë¡œ ìž‘ë™ ì¤‘ìž…ë‹ˆë‹¤!",
    }
    
    response_text = "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    for keyword, response in responses.items():
        if keyword in message:
            response_text = response
            break
    
    return {
        "response": response_text,
        "user_id": request.user_id,
        "timestamp": datetime.datetime.now().isoformat(),
        "server_info": {
            "type": "GPU Instance",
            "location": "asia-northeast3-b",
            "gpu_status": gpu_status
        }
    }

if __name__ == "__main__":
    import uvicorn
    print("ðŸš€ GPU ì±—ë´‡ ì„œë²„ ì‹œìž‘...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    uvicorn.run(app, host="0.0.0.0", port=8000)
PYTHON_CODE

# 5. ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo tee /etc/systemd/system/gpu-chatbot.service << 'SERVICE'
[Unit]
Description=GPU Chatbot Service
After=network.target

[Service]
Type=simple
User=isangjae
WorkingDirectory=/home/isangjae
Environment="PATH=/home/isangjae/chatbot_env/bin"
ExecStart=/home/isangjae/chatbot_env/bin/python /home/isangjae/gpu_chatbot.py
Restart=always

[Install]
WantedBy=multi-user.target
SERVICE

# 6. ì„œë¹„ìŠ¤ ì‹œìž‘
sudo systemctl daemon-reload
sudo systemctl enable gpu-chatbot
sudo systemctl start gpu-chatbot

echo "âœ… GPU ì±—ë´‡ ë°°í¬ ì™„ë£Œ!"
echo "í…ŒìŠ¤íŠ¸: curl http://34.22.80.119:8000/health"