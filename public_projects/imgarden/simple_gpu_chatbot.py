from fastapi import FastAPI
from fastapi.responses import JSONResponse
import torch
import uvicorn
import datetime

app = FastAPI()

@app.get("/")
def root():
    return {"message": "GPU ì±—ë´‡ ì„œë²„ì…ë‹ˆë‹¤"}

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "message": "GPU ì„œë²„ì—ì„œ ì‹¤í–‰ ì¤‘!",
        "server": "Google Cloud GPU Instance (T4)",
        "pytorch_version": torch.__version__,
        "cuda_available": torch.cuda.is_available(),
        "timestamp": datetime.datetime.now().isoformat()
    }

@app.post("/chat")
async def chat(request: dict):
    message = request.get("message", "")
    user_id = request.get("user_id", "guest")
    
    # GPUì—ì„œ ê°„ë‹¨í•œ ì—°ì‚° ìˆ˜í–‰ (ë°ëª¨ìš©)
    if torch.cuda.is_available():
        device = torch.device("cuda")
        tensor = torch.randn(100, 100).to(device)
        result = torch.matmul(tensor, tensor)
        gpu_used = "NVIDIA Tesla T4 GPU ì‚¬ìš©"
    else:
        gpu_used = "CPU ì‚¬ìš© (GPU ë¯¸ê°ì§€)"
    
    responses = {
        "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! GPU ì„œë²„ì—ì„œ ì¸ì‚¬ë“œë¦½ë‹ˆë‹¤! ğŸš€",
        "ì ì‹¬": "GPU ì„œë²„ê°€ ì¶”ì²œí•˜ëŠ” ì˜¤ëŠ˜ì˜ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œì…ë‹ˆë‹¤!",
        "í•œì‹": "ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ëœì¥ì°Œê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤!",
    }
    
    # í‚¤ì›Œë“œ ë§¤ì¹­
    response_text = "GPU ì„œë²„ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    for keyword, response in responses.items():
        if keyword in message:
            response_text = response
            break
    
    return {
        "response": response_text,
        "user_id": user_id,
        "server_type": "GCP GPU Instance",
        "gpu_info": gpu_used,
        "timestamp": datetime.datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("ğŸš€ GPU ì±—ë´‡ ì„œë²„ ì‹œì‘...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    uvicorn.run(app, host="0.0.0.0", port=8000)