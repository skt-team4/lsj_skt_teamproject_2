#!/bin/bash

# A.X-3.1-Light ëª¨ë¸ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
echo "ðŸš€ A.X-3.1-Light ëª¨ë¸ GPU ë°°í¬ ì‹œìž‘..."

SERVER_IP="34.22.80.119"

# 1. í”„ë¡œì íŠ¸ íŒŒì¼ ì••ì¶•
echo "ðŸ“¦ í”„ë¡œì íŠ¸ íŒŒì¼ ì••ì¶• ì¤‘..."
tar -czf naviyam_ax_deploy.tar.gz \
    --exclude='*.pyc' \
    --exclude='__pycache__' \
    --exclude='.git' \
    --exclude='cache/models/*' \
    ../naviyam_backend_runnable_20250806_112316/

# 2. GPU ì„œë²„ë¡œ ì „ì†¡
echo "ðŸ“¤ GPU ì„œë²„ë¡œ íŒŒì¼ ì „ì†¡ ì¤‘..."
scp naviyam_ax_deploy.tar.gz isangjae@${SERVER_IP}:/home/isangjae/

# 3. GPU ì„œë²„ì—ì„œ ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > setup_ax_model.sh << 'SETUP_SCRIPT'
#!/bin/bash

echo "ðŸ”§ A.X-3.1-Light ëª¨ë¸ ì„¤ì • ì‹œìž‘..."

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop ax-chatbot 2>/dev/null
docker rm ax-chatbot 2>/dev/null

# í”„ë¡œì íŠ¸ ì••ì¶• í•´ì œ
cd /home/isangjae
rm -rf naviyam_backend
tar -xzf naviyam_ax_deploy.tar.gz
mv naviyam_backend_runnable_20250806_112316 naviyam_backend

# Dockerfile ìƒì„±
cat > /home/isangjae/naviyam_backend/Dockerfile << 'DOCKERFILE'
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

# Python 3.10 ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3-pip \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# ìž‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# í”„ë¡œì íŠ¸ íŒŒì¼ ë³µì‚¬
COPY . /app/

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ (4bit ì–‘ìží™” í¬í•¨)
RUN pip3 install --no-cache-dir \
    torch==2.8.0 \
    transformers==4.48.0 \
    accelerate==1.2.1 \
    bitsandbytes==0.45.1 \
    fastapi==0.115.6 \
    uvicorn==0.34.0 \
    faiss-cpu==1.9.0.post1 \
    numpy==1.26.4 \
    scikit-learn==1.6.1 \
    pandas==2.2.3 \
    sentencepiece==0.2.0 \
    protobuf==5.29.3 \
    peft==0.14.0 \
    GPUtil==1.4.0

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV USE_4BIT=true
ENV MODEL_NAME=skt/A.X-3.1-Light
ENV CUDA_VISIBLE_DEVICES=0

# í¬íŠ¸ ì„¤ì •
EXPOSE 8000

# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ë° ì„œë²„ ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸
RUN echo '#!/bin/bash' > /app/start.sh && \
    echo 'echo "ðŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œìž‘..."' >> /app/start.sh && \
    echo 'python3 -c "from transformers import AutoModelForCausalLM, AutoTokenizer; import torch; print(\"Downloading model...\"); tokenizer = AutoTokenizer.from_pretrained(\"skt/A.X-3.1-Light\"); model = AutoModelForCausalLM.from_pretrained(\"skt/A.X-3.1-Light\", load_in_4bit=True, device_map=\"auto\", torch_dtype=torch.float16); print(\"Model loaded successfully!\")"' >> /app/start.sh && \
    echo 'echo "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì„œë²„ ì‹œìž‘..."' >> /app/start.sh && \
    echo 'python3 -m uvicorn api.server:app --host 0.0.0.0 --port 8000' >> /app/start.sh && \
    chmod +x /app/start.sh

CMD ["/app/start.sh"]
DOCKERFILE

# models_config.py ìˆ˜ì • (4bit ì–‘ìží™” ê°•ì œ í™œì„±í™”)
cat > /home/isangjae/naviyam_backend/models/models_config.py << 'PYTHON_CONFIG'
import os
import torch

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIGS = {
    "ax": {
        "model_name": "skt/A.X-3.1-Light",
        "model_class": "AXModel",
        "max_length": 2048,
        "temperature": 0.7,
        "top_p": 0.9,
        "use_4bit": True,  # 4bit ì–‘ìží™” ê°•ì œ í™œì„±í™”
        "device_map": "auto",
        "torch_dtype": torch.float16,
        "load_in_4bit": True,
        "bnb_4bit_compute_dtype": torch.float16,
        "bnb_4bit_use_double_quant": True,
        "bnb_4bit_quant_type": "nf4"
    }
}

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
DEFAULT_MODEL = "ax"

# GPU ì„¤ì •
USE_CUDA = torch.cuda.is_available()
DEVICE = torch.device("cuda:0" if USE_CUDA else "cpu")

print(f"ðŸ”§ Models Config Loaded")
print(f"  - USE_CUDA: {USE_CUDA}")
print(f"  - DEVICE: {DEVICE}")
print(f"  - 4bit Quantization: ENABLED")
print(f"  - Model: {MODEL_CONFIGS['ax']['model_name']}")
PYTHON_CONFIG

# ax_model.py ìˆ˜ì • (4bit ì–‘ìží™” ë³´ìž¥)
cat > /home/isangjae/naviyam_backend/models/ax_model.py << 'PYTHON_MODEL'
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import logging

logger = logging.getLogger(__name__)

class AXModel:
    def __init__(self, model_config):
        self.config = model_config
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"Initializing A.X-3.1-Light with 4bit quantization")
        logger.info(f"Device: {self.device}")
        
    def load(self):
        """ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìží™”)"""
        try:
            model_name = self.config.get("model_name", "skt/A.X-3.1-Light")
            
            # 4bit ì–‘ìží™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            logger.info(f"Loading tokenizer from {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"Loading model with 4bit quantization...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                logger.info(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
            
            logger.info("âœ… Model loaded successfully with 4bit quantization!")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def generate(self, prompt, max_length=512, temperature=0.7, top_p=0.9):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            # ìž…ë ¥ í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=1024
            )
            
            # GPUë¡œ ì´ë™
            input_ids = inputs["input_ids"].to(self.model.device)
            attention_mask = inputs["attention_mask"].to(self.model.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            return response
            
        except Exception as e:
            logger.error(f"Generation error: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        torch.cuda.empty_cache()
        logger.info("Model unloaded and GPU memory cleared")
PYTHON_MODEL

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "ðŸ”¨ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘..."
cd /home/isangjae/naviyam_backend
docker build -t ax-chatbot:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (ì¶©ë¶„í•œ ì‹œê°„ ì œê³µ)
echo "ðŸš€ ì»¨í…Œì´ë„ˆ ì‹œìž‘ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)..."
docker run -d \
    --name ax-chatbot \
    --gpus all \
    -p 8000:8000 \
    -e USE_4BIT=true \
    -e MODEL_NAME="skt/A.X-3.1-Light" \
    -e CUDA_VISIBLE_DEVICES=0 \
    -v /home/isangjae/model_cache:/root/.cache/huggingface \
    ax-chatbot:latest

echo "â³ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ëŒ€ê¸° ì¤‘ (ì•½ 5-10ë¶„)..."
sleep 30

# ë¡œê·¸ í™•ì¸
echo "ðŸ“‹ ì»¨í…Œì´ë„ˆ ë¡œê·¸:"
docker logs --tail 50 ax-chatbot

echo "âœ… ë°°í¬ ì™„ë£Œ!"
echo "í…ŒìŠ¤íŠ¸: curl http://localhost:8000/health"
SETUP_SCRIPT

# 4. ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì „ì†¡ ë° ì‹¤í–‰
echo "ðŸ“¤ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì „ì†¡ ì¤‘..."
scp setup_ax_model.sh isangjae@${SERVER_IP}:/home/isangjae/

echo "ðŸ”§ GPU ì„œë²„ì—ì„œ ì„¤ì • ì‹¤í–‰ ì¤‘..."
ssh isangjae@${SERVER_IP} "chmod +x /home/isangjae/setup_ax_model.sh && /home/isangjae/setup_ax_model.sh"

echo "âœ… ë°°í¬ ì™„ë£Œ!"
echo "í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:"
echo "  curl http://${SERVER_IP}:8000/health"
echo "  python3 gpu_chat.py"