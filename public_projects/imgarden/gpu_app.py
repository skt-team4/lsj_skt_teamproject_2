from fastapi import FastAPI
from pydantic import BaseModel
import torch
import datetime
import uvicorn

app = FastAPI(title="GPU ì±—ë´‡ ì„œë²„")

class ChatRequest(BaseModel):
    message: str
    user_id: str = "guest"

@app.get("/")
def root():
    return {"service": "GPU ì±—ë´‡ ì„œë²„", "status": "running"}

@app.get("/health")
def health():
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "device_name": torch.cuda.get_device_name(0),
            "memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB",
            "cuda_version": torch.version.cuda
        }
    else:
        gpu_info = {"available": False}
        
    return {
        "status": "healthy",
        "message": "GPU ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "gpu": gpu_info,
        "pytorch_version": torch.__version__
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    # GPUì—ì„œ ê°„ë‹¨í•œ ì—°ì‚° ìˆ˜í–‰
    if torch.cuda.is_available():
        device = torch.device("cuda")
        # GPU í…ŒìŠ¤íŠ¸ ì—°ì‚°
        tensor = torch.randn(1000, 1000).to(device)
        result = torch.matmul(tensor, tensor)
        gpu_status = f"GPU ì‚¬ìš© ì¤‘ ({torch.cuda.get_device_name(0)})"
    else:
        gpu_status = "CPU ëª¨ë“œ"
    
    responses = {
        "ì•ˆë…•": "ì•ˆë…•í•˜ì„¸ìš”! NVIDIA L4 GPUë¡œ êµ¬ë™ë˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤! ğŸš€",
        "ì ì‹¬": "GPUê°€ ì¶”ì²œí•˜ëŠ” ì˜¤ëŠ˜ì˜ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œì…ë‹ˆë‹¤!",
        "í•œì‹": "ë¹„ë¹”ë°¥, ë¶ˆê³ ê¸°, ëœì¥ì°Œê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤!",
        "gpu": f"í˜„ì¬ {gpu_status}ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
    }
    
    response_text = "GPU ì„œë²„ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    for keyword, response in responses.items():
        if keyword in request.message.lower():
            response_text = response
            break
    
    return {
        "response": response_text,
        "user_id": request.user_id,
        "gpu_status": gpu_status,
        "timestamp": datetime.datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("ğŸš€ GPU ì±—ë´‡ ì„œë²„ ì‹œì‘...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
    uvicorn.run(app, host="0.0.0.0", port=8000)