#!/bin/bash

# GPU ì„œë²„ì—ì„œ ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸
echo "ğŸš€ GPU ì„œë²„ì— ì±—ë´‡ ë°°í¬ ì¤‘..."

# Docker ì´ë¯¸ì§€ pull (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€)
echo "Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ..."
sudo docker pull pytorch/pytorch:2.3.0-cuda11.8-cudnn8-runtime

# ì±—ë´‡ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
echo "ì±—ë´‡ ì»¨í…Œì´ë„ˆ ì‹œì‘..."
sudo docker run -d \
  --name naviyam-gpu-test \
  --gpus all \
  -p 8000:8000 \
  -e CUDA_VISIBLE_DEVICES=0 \
  pytorch/pytorch:2.3.0-cuda11.8-cudnn8-runtime \
  python -c "
import torch
from http.server import HTTPServer, BaseHTTPRequestHandler
import json

class ChatbotHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            
            # GPU ì •ë³´ í™•ì¸
            gpu_available = torch.cuda.is_available()
            gpu_name = torch.cuda.get_device_name(0) if gpu_available else 'No GPU'
            gpu_memory = f'{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB' if gpu_available else 'N/A'
            
            response = {
                'status': 'healthy',
                'message': 'GPU ì„œë²„ì—ì„œ ì‹¤í–‰ ì¤‘!',
                'gpu': {
                    'available': gpu_available,
                    'name': gpu_name,
                    'memory': gpu_memory
                }
            }
            self.wfile.write(json.dumps(response).encode())
    
    def do_POST(self):
        if self.path == '/chat':
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data)
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            
            # GPUì—ì„œ ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° ìˆ˜í–‰
            if torch.cuda.is_available():
                device = torch.device('cuda')
                tensor = torch.randn(1000, 1000).to(device)
                result = torch.matmul(tensor, tensor)
                gpu_used = True
            else:
                gpu_used = False
            
            response = {
                'response': f'GPU ì„œë²„ì…ë‹ˆë‹¤! ë©”ì‹œì§€ \"{data.get(\"message\", \"\")}\" ë°›ì•˜ìŠµë‹ˆë‹¤.',
                'gpu_used': gpu_used,
                'server_type': 'GPU Instance (T4)'
            }
            self.wfile.write(json.dumps(response).encode())

print('ğŸš€ GPU ì±—ë´‡ ì„œë²„ ì‹œì‘ (í¬íŠ¸ 8000)...')
print(f'GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU ì´ë¦„: {torch.cuda.get_device_name(0)}')

server = HTTPServer(('0.0.0.0', 8000), ChatbotHandler)
server.serve_forever()
"

echo "âœ… ë°°í¬ ì™„ë£Œ!"
echo "í…ŒìŠ¤íŠ¸: curl http://localhost:8000/health"