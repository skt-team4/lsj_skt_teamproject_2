#!/usr/bin/env python3
"""
A.X-3.1-Light 4bit ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì„œë²„
"""
from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import datetime
import uvicorn
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="A.X-3.1-Light 4bit Server")

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None

class ChatRequest(BaseModel):
    message: str
    user_id: str = "guest"

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global model, tokenizer
    
    logger.info("ğŸš€ A.X-3.1-Light ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    
    try:
        # 4bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model_name = "skt/A.X-3.1-Light"
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
        logger.info("ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"âœ… GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB")
        
        logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! 4bit ì–‘ìí™” í™œì„±í™”ë¨")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise

@app.get("/")
def root():
    return {
        "service": "A.X-3.1-Light 4bit Server",
        "status": "running",
        "model_loaded": model is not None
    }

@app.get("/health")
def health():
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "device_name": torch.cuda.get_device_name(0),
            "memory_allocated": f"{torch.cuda.memory_allocated(0) / 1e9:.2f} GB",
            "memory_reserved": f"{torch.cuda.memory_reserved(0) / 1e9:.2f} GB",
            "memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB",
            "cuda_version": torch.version.cuda
        }
    else:
        gpu_info = {"available": False}
    
    return {
        "status": "healthy",
        "message": "4bit ì–‘ìí™” ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "gpu": gpu_info,
        "pytorch_version": torch.__version__,
        "model_loaded": model is not None,
        "quantization": "4bit (nf4)"
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    """ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    if model is None or tokenizer is None:
        return {
            "response": "ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "error": True
        }
    
    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ìì™€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ëŒ€í™”ì…ë‹ˆë‹¤.

ì‚¬ìš©ì: {request.message}
ì–´ì‹œìŠ¤í„´íŠ¸:"""
        
        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # GPUë¡œ ì´ë™
        input_ids = inputs["input_ids"].to(model.device)
        attention_mask = inputs["attention_mask"].to(model.device)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°
        if "ì–´ì‹œìŠ¤í„´íŠ¸:" in response:
            response = response.split("ì–´ì‹œìŠ¤í„´íŠ¸:")[-1].strip()
        
        # GPU ìƒíƒœ
        gpu_status = "4bit ì–‘ìí™” GPU ëª¨ë“œ"
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_status = f"4bit GPU ({allocated:.1f}GB ì‚¬ìš©)"
        
        return {
            "response": response,
            "user_id": request.user_id,
            "gpu_status": gpu_status,
            "quantization": "4bit",
            "timestamp": datetime.datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "error": str(e)
        }

if __name__ == "__main__":
    print("ğŸš€ A.X-3.1-Light 4bit ì„œë²„ ì‹œì‘...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
    uvicorn.run(app, host="0.0.0.0", port=8000)