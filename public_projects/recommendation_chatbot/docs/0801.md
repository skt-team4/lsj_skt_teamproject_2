# LLM 기반 답변 생성(NLG)의 현실적 개선 전략 (2025-08-01)

LLM의 창의성을 유지하면서, 운영 환경의 안정성과 비용 효율성을 확보하기 위한 두 가지 핵심 전략.

---

### 1. '안전망 (Safety Net)' 도입을 통한 위험 통제

**문제점:** LLM이 생성하는 답변은 통제 불가능하며, 부적절하거나 잘못된 정보를 포함할 수 있다. 특히 아동 대상 서비스에서는 치명적인 리스크가 될 수 있다.

**해결 방안:** LLM이 생성한 답변을 사용자에게 보내기 전, 3단계의 자동 검증 시스템을 통과하도록 강제한다.

1.  **1단계 (생성):** LLM이 '아동 친화적' 답변의 초안을 생성한다.
2.  **2단계 (자동 검증):** 시스템이 생성된 초안을 아래 기준으로 자동 검사한다.
    *   **유해 단어 필터:** 설정된 비속어, 부적절한 단어 목록이 포함되었는지 검사.
    *   **필수 정보 체크:** 추천 가게 이름, 핵심 메뉴 등 반드시 포함되어야 할 정보가 누락되지 않았는지 검사.
    *   **문맥 이탈 체크:** 사용자의 질문 의도와 완전히 동떨어진 답변을 생성하지 않았는지 간단한 논리 규칙으로 검사.
3.  **3단계 (결정):**
    *   **(통과)** 모든 검증을 통과한 경우에만, 안전성이 확보된 답변을 사용자에게 전달한다.
    *   **(실패)** 단 하나의 검증이라도 실패하면, 즉시 **미리 준비된 안전한 템플릿 답변으로 대체**하여 응답의 안정성을 보장한다.

**기대 효과:** LLM의 창의성을 최대한 활용하면서도, 통제 불가능한 리스크가 사용자에게 노출되는 것을 원천적으로 차단할 수 있다.

---

### 2. '지능형 캐싱 (Intelligent Caching)'을 통한 비용 절감

**문제점:** 모든 요청에 대해 실시간으로 LLM을 호출하는 것은 막대한 비용을 유발하며, 서비스 확장 시 가장 큰 걸림돌이 된다.

**해결 방안:** 자주 사용되는 답변을 미리 생성하여 저장해두고, 필요할 때 꺼내 쓰는 캐싱 전략을 도입한다.

1.  **사전 생성 (Offline):** 자주 추천되는 상위 아이템(예: 인기 가게 Top 20)에 대해, LLM을 이용해 **"A+급 모범 추천 문장"을 각각 10개씩 미리 생성**해 둔다.
2.  **캐시 저장:** 생성된 총 200개(20개 x 10개)의 모범 답변을 **캐시(Cache) 데이터베이스에 저장**한다.
3.  **실시간 응답:**
    *   사용자가 캐시된 가게를 추천받아야 할 경우, 비싼 LLM을 실시간으로 호출하는 대신, 캐시에 저장된 **10개의 모범 답변 중 하나를 랜덤으로 선택하여 즉시 보여준다.**
    *   캐시에 없는 가게를 추천해야 하거나, 사용자의 질문이 매우 독특하여 미리 대비할 수 없었던 **예외적인 경우에만 제한적으로 실시간 LLM을 호출**한다.

**기대 효과:**
*   사용자 입장에서는 매번 AI가 새롭게 답변을 생성해주는 듯한 동적인 경험을 유지할 수 있다.
*   실제로는 **전체 LLM 호출의 80~90%를 줄여 비용을 획기적으로 절감**하고, 응답 속도 또한 크게 향상시킬 수 있다.

---

### 결론

두 전략의 결합을 통해, **"스스로 생각하고 말하는 AI"**라는 프로젝트의 핵심 비전을 유지하면서도, **비용과 안전성**이라는 현실적인 운영 문제를 해결하여 지속 가능한 서비스로 발전시킬 수 있다.
