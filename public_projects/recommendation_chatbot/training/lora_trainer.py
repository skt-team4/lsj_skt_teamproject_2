"""
ë‚˜ë¹„ì–Œ LoRA ì–´ëŒ‘í„° í›ˆë ¨ ì‹œìŠ¤í…œ
ì‹¤ì‹œê°„ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê³¼ ì—°ë™ëœ ì ì§„ì  í•™ìŠµ
"""

import torch
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import queue
import threading
import time

from datasets import Dataset
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, TaskType

from data.data_structure import NaviyamKnowledge, LearningData
from models.koalpaca_model import KoAlpacaModel
from inference.data_collector import LearningDataCollector

logger = logging.getLogger(__name__)

@dataclass
class LoRATrainingConfig:
    """LoRA í›ˆë ¨ ì„¤ì •"""
    # LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: List[str] = None
    
    # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    learning_rate: float = 1e-4
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    epochs: int = 3
    warmup_steps: int = 100
    
    # ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •
    min_samples_for_training: int = 50
    max_samples_per_batch: int = 200
    quality_threshold: float = 0.7
    
    # ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
    training_interval_hours: int = 6
    auto_training_enabled: bool = True
    max_daily_trainings: int = 4
    
    def __post_init__(self):
        if self.target_modules is None:
            # KoAlpaca ê¸°ë³¸ íƒ€ê²Ÿ ëª¨ë“ˆ
            self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

class NaviyamLoRATrainer:
    """ë‚˜ë¹„ì–Œ LoRA ì–´ëŒ‘í„° í›ˆë ¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self, model: KoAlpacaModel, config: LoRATrainingConfig, data_collector: LearningDataCollector):
        """
        Args:
            model: KoAlpaca ëª¨ë¸
            config: LoRA í›ˆë ¨ ì„¤ì •
            data_collector: í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸°
        """
        self.model = model
        self.config = config
        self.data_collector = data_collector
        
        # í›ˆë ¨ ìƒíƒœ
        self.is_training = False
        self.last_training_time = None
        self.training_history = []
        self.current_adapter_version = 0
        
        # ë¹„ë™ê¸° í›ˆë ¨ í
        self.training_queue = queue.Queue()
        self.training_executor = ThreadPoolExecutor(max_workers=1)
        self.training_thread = None
        
        # ì–´ëŒ‘í„° ê´€ë¦¬
        self.active_adapters = {}  # {adapter_name: adapter_path}
        self.adapter_performance = {}  # {adapter_name: performance_metrics}
        
        # ì €ì¥ ê²½ë¡œ
        self.base_save_path = Path("./models/lora_adapters")
        self.base_save_path.mkdir(parents=True, exist_ok=True)
        
        logger.info("ë‚˜ë¹„ì–Œ LoRA í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_auto_training(self):
        """ìë™ í›ˆë ¨ ì‹œì‘"""
        if not self.config.auto_training_enabled:
            logger.info("ìë™ í›ˆë ¨ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
            
        if self.training_thread and self.training_thread.is_alive():
            logger.warning("ì´ë¯¸ ìë™ í›ˆë ¨ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
            
        self.training_thread = threading.Thread(target=self._auto_training_loop, daemon=True)
        self.training_thread.start()
        logger.info("ìë™ LoRA í›ˆë ¨ ì‹œì‘")
    
    def stop_auto_training(self):
        """ìë™ í›ˆë ¨ ì¤‘ì§€"""
        self.config.auto_training_enabled = False
        if self.training_thread:
            self.training_thread.join(timeout=5)
        logger.info("ìë™ LoRA í›ˆë ¨ ì¤‘ì§€")
    
    def _auto_training_loop(self):
        """ìë™ í›ˆë ¨ ë£¨í”„"""
        logger.info("ìë™ í›ˆë ¨ ë£¨í”„ ì‹œì‘")
        
        while self.config.auto_training_enabled:
            try:
                # í›ˆë ¨ ì¡°ê±´ í™•ì¸
                if self._should_trigger_training():
                    logger.info("í›ˆë ¨ ì¡°ê±´ ì¶©ì¡± - ìë™ í›ˆë ¨ ì‹œì‘")
                    
                    # ë¹„ë™ê¸°ë¡œ í›ˆë ¨ ì‹¤í–‰
                    future = self.training_executor.submit(self._execute_auto_training)
                    
                    # í›ˆë ¨ ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                    try:
                        result = future.result(timeout=3600)  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
                        logger.info(f"ìë™ í›ˆë ¨ ì™„ë£Œ: {result}")
                    except Exception as e:
                        logger.error(f"ìë™ í›ˆë ¨ ì‹¤íŒ¨: {e}")
                
                # ë‹¤ìŒ ì²´í¬ê¹Œì§€ ëŒ€ê¸° (30ë¶„)
                time.sleep(1800)
                
            except Exception as e:
                logger.error(f"ìë™ í›ˆë ¨ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    def _should_trigger_training(self) -> bool:
        """í›ˆë ¨ ì‹¤í–‰ ì¡°ê±´ í™•ì¸"""
        # ì´ë¯¸ í›ˆë ¨ ì¤‘ì´ë©´ ìŠ¤í‚µ
        if self.is_training:
            return False
        
        # ì¼ì¼ í›ˆë ¨ íšŸìˆ˜ í™•ì¸
        today = datetime.now().date()
        today_trainings = [
            t for t in self.training_history 
            if t['timestamp'].date() == today
        ]
        if len(today_trainings) >= self.config.max_daily_trainings:
            logger.info(f"ì¼ì¼ ìµœëŒ€ í›ˆë ¨ íšŸìˆ˜ ì´ˆê³¼: {len(today_trainings)}/{self.config.max_daily_trainings}")
            return False
        
        # ë§ˆì§€ë§‰ í›ˆë ¨ ì‹œê°„ í™•ì¸
        if self.last_training_time:
            time_since_last = datetime.now() - self.last_training_time
            if time_since_last < timedelta(hours=self.config.training_interval_hours):
                return False
        
        # ìˆ˜ì§‘ëœ ë°ì´í„° í™•ì¸
        stats = self.data_collector.get_collection_statistics()
        total_samples = stats['quality_stats']['valid_samples']
        
        if total_samples < self.config.min_samples_for_training:
            logger.debug(f"í›ˆë ¨ ë°ì´í„° ë¶€ì¡±: {total_samples}/{self.config.min_samples_for_training}")
            return False
        
        logger.info(f"í›ˆë ¨ ì¡°ê±´ ì¶©ì¡±: {total_samples}ê°œ ìƒ˜í”Œ, {time_since_last if self.last_training_time else 'first time'}")
        return True
    
    def _execute_auto_training(self) -> Dict[str, Any]:
        """ìë™ í›ˆë ¨ ì‹¤í–‰"""
        try:
            # ìµœì‹  í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            training_data = self._collect_training_data()
            
            if len(training_data) < self.config.min_samples_for_training:
                return {"status": "skipped", "reason": "insufficient_data", "samples": len(training_data)}
            
            # LoRA ì–´ëŒ‘í„° í›ˆë ¨
            adapter_name = f"auto_v{self.current_adapter_version}_{datetime.now().strftime('%Y%m%d_%H%M')}"
            
            result = self.train_lora_adapter(
                adapter_name=adapter_name,
                training_data=training_data,
                save_best=True
            )
            
            # ì„±ëŠ¥ í‰ê°€
            performance = self._evaluate_adapter_performance(adapter_name)
            result.update(performance)
            
            # ê¸°ì¡´ ì–´ëŒ‘í„°ì™€ ì„±ëŠ¥ ë¹„êµ
            if self._should_deploy_new_adapter(adapter_name, performance):
                self._deploy_adapter(adapter_name)
                result["deployed"] = True
            else:
                result["deployed"] = False
                logger.info(f"ìƒˆ ì–´ëŒ‘í„° ì„±ëŠ¥ ë¶€ì¡± - ë°°í¬ ìŠ¤í‚µ: {adapter_name}")
            
            return result
            
        except Exception as e:
            logger.error(f"ìë™ í›ˆë ¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"status": "failed", "error": str(e)}
    
    def train_lora_adapter(self, adapter_name: str, training_data: List[Dict], 
                          save_best: bool = True) -> Dict[str, Any]:
        """LoRA ì–´ëŒ‘í„° í›ˆë ¨"""
        logger.info(f"LoRA ì–´ëŒ‘í„° í›ˆë ¨ ì‹œì‘: {adapter_name} ({len(training_data)}ê°œ ìƒ˜í”Œ)")
        
        self.is_training = True
        training_start_time = datetime.now()
        
        try:
            # LoRA ì„¤ì •
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=self.config.target_modules,
                bias="none",
                inference_mode=False,
            )
            
            # PEFT ëª¨ë¸ ìƒì„± (ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆë‹¤ë©´ ì œê±° í›„ ìƒˆë¡œ ìƒì„±)
            if hasattr(self.model.model, 'peft_config'):
                # ê¸°ì¡´ ì–´ëŒ‘í„° ì œê±°
                self.model.model = self.model.model.base_model.model
            
            peft_model = get_peft_model(self.model.model, lora_config)
            
            # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            train_dataset, eval_dataset = self._prepare_datasets(training_data)
            
            # í›ˆë ¨ ì„¤ì •
            training_args = self._get_training_arguments(adapter_name)
            
            # ë°ì´í„° ì½œë ˆì´í„°
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.model.tokenizer,
                mlm=False,
                pad_to_multiple_of=8
            )
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            trainer = Trainer(
                model=peft_model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                tokenizer=self.model.tokenizer,
            )
            
            # í›ˆë ¨ ì‹¤í–‰
            logger.info(f"í›ˆë ¨ ì‹œì‘: {len(train_dataset)}ê°œ í•™ìŠµ, {len(eval_dataset)}ê°œ ê²€ì¦")
            train_result = trainer.train()
            
            # ì–´ëŒ‘í„° ì €ì¥
            adapter_path = self.base_save_path / adapter_name
            adapter_path.mkdir(parents=True, exist_ok=True)
            
            peft_model.save_pretrained(str(adapter_path))
            
            # í›ˆë ¨ ê²°ê³¼ ì €ì¥
            training_duration = datetime.now() - training_start_time
            
            result = {
                "adapter_name": adapter_name,
                "status": "completed",
                "training_loss": train_result.training_loss,
                "training_duration_minutes": training_duration.total_seconds() / 60,
                "training_samples": len(train_dataset),
                "eval_samples": len(eval_dataset),
                "adapter_path": str(adapter_path),
                "timestamp": training_start_time
            }
            
            # í™œì„± ì–´ëŒ‘í„° ëª©ë¡ì— ì¶”ê°€
            self.active_adapters[adapter_name] = str(adapter_path)
            
            # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.training_history.append(result)
            self.last_training_time = training_start_time
            self.current_adapter_version += 1
            
            # ëª¨ë¸ì— ìƒˆ ì–´ëŒ‘í„° ë¡œë“œ
            self.model.model = peft_model
            
            logger.info(f"LoRA ì–´ëŒ‘í„° í›ˆë ¨ ì™„ë£Œ: {adapter_name} (ì†ì‹¤: {train_result.training_loss:.4f})")
            
            return result
            
        except Exception as e:
            logger.error(f"LoRA ì–´ëŒ‘í„° í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise
        finally:
            self.is_training = False
    
    def _collect_training_data(self) -> List[Dict]:
        """í›ˆë ¨ìš© ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸°ì—ì„œ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_data = self.data_collector.get_recent_data(days=7)  # ìµœê·¼ 7ì¼ ë°ì´í„°
        
        # í’ˆì§ˆ í•„í„°ë§
        quality_data = []
        for data in all_data:
            if self._is_high_quality_data(data):
                quality_data.append(data)
        
        # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if len(quality_data) > self.config.max_samples_per_batch:
            # ìµœì‹  ë°ì´í„° ìš°ì„ ìœ¼ë¡œ ìƒ˜í”Œë§
            quality_data = sorted(quality_data, key=lambda x: x.get('timestamp', datetime.min), reverse=True)
            quality_data = quality_data[:self.config.max_samples_per_batch]
        
        # í›ˆë ¨ í˜•íƒœë¡œ ë³€í™˜
        training_samples = []
        for data in quality_data:
            sample = self._convert_to_training_sample(data)
            if sample:
                training_samples.append(sample)
        
        logger.info(f"í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(training_samples)}ê°œ ìƒ˜í”Œ")
        return training_samples
    
    def _is_high_quality_data(self, data: Dict) -> bool:
        """ë°ì´í„° í’ˆì§ˆ í™•ì¸"""
        # ê¸°ë³¸ í•„ë“œ í™•ì¸
        if not data.get('user_input') or not data.get('bot_response'):
            return False
        
        # ê¸¸ì´ í™•ì¸
        if len(data['user_input']) < 3 or len(data['bot_response']) < 5:
            return False
        
        # í’ˆì§ˆ ì ìˆ˜ í™•ì¸
        quality_score = data.get('quality_score', 0.0)
        if quality_score < self.config.quality_threshold:
            return False
        
        # íŠ¹ì • íŒ¨í„´ ì œì™¸ (ì˜¤ë¥˜ ë©”ì‹œì§€ ë“±)
        error_patterns = ["ì˜¤ë¥˜", "ì‹¤íŒ¨", "ì£„ì†¡", "ëª¨ë¥´ê² "]
        bot_response = data['bot_response'].lower()
        if any(pattern in bot_response for pattern in error_patterns):
            return False
        
        return True
    
    def _convert_to_training_sample(self, data: Dict) -> Optional[Dict]:
        """ë°ì´í„°ë¥¼ í›ˆë ¨ ìƒ˜í”Œë¡œ ë³€í™˜"""
        try:
            user_input = data['user_input']
            bot_response = data['bot_response']
            
            # ë‚˜ë¹„ì–Œ í”„ë¡¬í”„íŠ¸ í˜•ì‹
            prompt_template = """ë‹¹ì‹ ì€ ë‚˜ë¹„ì–Œ, ì•„ë™ì„ ìœ„í•œ ì°©í•œê°€ê²Œ ì¶”ì²œ AIì…ë‹ˆë‹¤.
ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ìŒì‹ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì: {user_input}
ë‚˜ë¹„ì–Œ: {bot_response}"""
            
            formatted_text = prompt_template.format(
                user_input=user_input,
                bot_response=bot_response
            )
            
            # í† í¬ë‚˜ì´ì§•
            tokens = self.model.tokenizer(
                formatted_text,
                truncation=True,
                padding=False,
                max_length=512,
                return_tensors=None
            )
            
            return {
                "input_ids": tokens["input_ids"],
                "attention_mask": tokens["attention_mask"],
                "labels": tokens["input_ids"].copy()  # Causal LMì—ì„œëŠ” inputê³¼ labelì´ ë™ì¼
            }
            
        except Exception as e:
            logger.warning(f"í›ˆë ¨ ìƒ˜í”Œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _prepare_datasets(self, training_data: List[Dict]) -> Tuple[Dataset, Dataset]:
        """í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„"""
        # í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ë§Œ í•„í„°ë§
        valid_data = [sample for sample in training_data if sample is not None]
        
        # 8:2 ë¶„í• 
        split_idx = int(len(valid_data) * 0.8)
        train_data = valid_data[:split_idx]
        eval_data = valid_data[split_idx:] if split_idx < len(valid_data) else valid_data[-5:]  # ìµœì†Œ 5ê°œ í‰ê°€ ìƒ˜í”Œ
        
        train_dataset = Dataset.from_list(train_data)
        eval_dataset = Dataset.from_list(eval_data)
        
        return train_dataset, eval_dataset
    
    def _get_training_arguments(self, adapter_name: str) -> TrainingArguments:
        """í›ˆë ¨ ì¸ì ì„¤ì •"""
        output_dir = self.base_save_path / f"{adapter_name}_checkpoints"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        return TrainingArguments(
            output_dir=str(output_dir),
            
            # ê¸°ë³¸ ì„¤ì •
            num_train_epochs=self.config.epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            
            # í•™ìŠµë¥ 
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            
            # ë¡œê¹… ë° ì €ì¥
            logging_steps=10,
            save_steps=50,
            eval_steps=50,
            evaluation_strategy="steps",
            save_strategy="steps",
            
            # ìµœì í™”
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            
            # ë©”ëª¨ë¦¬ ìµœì í™”
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            fp16=torch.cuda.is_available(),
            
            # ê¸°íƒ€
            seed=42,
            report_to=None,
            save_total_limit=2,
        )
    
    def _evaluate_adapter_performance(self, adapter_name: str) -> Dict[str, float]:
        """ì–´ëŒ‘í„° ì„±ëŠ¥ í‰ê°€"""
        logger.info(f"ì–´ëŒ‘í„° ì„±ëŠ¥ í‰ê°€: {adapter_name}")
        
        try:
            # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            test_prompts = [
                {"input": "ì¹˜í‚¨ ë¨¹ê³  ì‹¶ì–´", "expected_keywords": ["ì¹˜í‚¨", "ì¶”ì²œ", "ê°€ê²Œ"]},
                {"input": "ì°©í•œê°€ê²Œ ì•Œë ¤ì¤˜", "expected_keywords": ["ì°©í•œê°€ê²Œ", "ì¶”ì²œ"]},
                {"input": "ë§Œì›ìœ¼ë¡œ ë­ ë¨¹ì„ê¹Œ", "expected_keywords": ["ë§Œì›", "ë©”ë‰´", "ì¶”ì²œ"]},
                {"input": "ê³ ë§ˆì›Œ", "expected_keywords": ["ì²œë§Œ", "ë§›ìˆê²Œ"]},
                {"input": "ì•ˆë…•", "expected_keywords": ["ì•ˆë…•", "ë°˜ê°€", "ë„ì›€"]}
            ]
            
            total_score = 0.0
            response_scores = []
            
            for prompt in test_prompts:
                try:
                    # ì‘ë‹µ ìƒì„±
                    response = self._generate_test_response(prompt["input"])
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                    keyword_score = self._calculate_keyword_score(response, prompt["expected_keywords"])
                    
                    # ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜
                    quality_score = self._calculate_response_quality(response)
                    
                    # ì¢…í•© ì ìˆ˜
                    combined_score = (keyword_score * 0.6 + quality_score * 0.4)
                    response_scores.append(combined_score)
                    total_score += combined_score
                    
                except Exception as e:
                    logger.warning(f"í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤íŒ¨: {e}")
                    response_scores.append(0.0)
            
            avg_score = total_score / max(len(test_prompts), 1)
            
            performance_metrics = {
                "overall_score": avg_score,
                "individual_scores": response_scores,
                "test_count": len(test_prompts),
                "evaluation_timestamp": datetime.now().isoformat()
            }
            
            # ì–´ëŒ‘í„° ì„±ëŠ¥ ì €ì¥
            self.adapter_performance[adapter_name] = performance_metrics
            
            logger.info(f"ì–´ëŒ‘í„° ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: {adapter_name} (ì ìˆ˜: {avg_score:.3f})")
            
            return performance_metrics
            
        except Exception as e:
            logger.error(f"ì–´ëŒ‘í„° ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {"overall_score": 0.0, "error": str(e)}
    
    def _generate_test_response(self, input_text: str) -> str:
        """í…ŒìŠ¤íŠ¸ìš© ì‘ë‹µ ìƒì„±"""
        try:
            prompt = f"ì‚¬ìš©ì: {input_text}\në‚˜ë¹„ì–Œ: "
            
            inputs = self.model.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=256
            )
            
            with torch.no_grad():
                outputs = self.model.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.model.tokenizer.eos_token_id
                )
            
            # ì…ë ¥ ë¶€ë¶„ ì œê±°í•˜ê³  ì‘ë‹µë§Œ ì¶”ì¶œ
            generated = self.model.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            logger.warning(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""
    
    def _calculate_keyword_score(self, response: str, expected_keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not response:
            return 0.0
        
        response_lower = response.lower()
        matched_keywords = sum(1 for keyword in expected_keywords if keyword in response_lower)
        
        return matched_keywords / max(len(expected_keywords), 1)
    
    def _calculate_response_quality(self, response: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not response:
            return 0.0
        
        score = 0.0
        
        # ê¸¸ì´ ì ì ˆì„± (10-100ì)
        length = len(response)
        if 10 <= length <= 100:
            score += 0.3
        elif 5 <= length <= 150:
            score += 0.2
        
        # ë‚˜ë¹„ì–Œ íŠ¹í™” í‘œí˜„
        naviyam_keywords = ["ì¶”ì²œ", "ê°€ê²Œ", "ë©”ë‰´", "ì°©í•œê°€ê²Œ", "ë§›ìˆ", "ë“œë¦´ê²Œìš”", "ì–´ë– ì„¸ìš”"]
        for keyword in naviyam_keywords:
            if keyword in response:
                score += 0.1
        
        # ì´ëª¨í‹°ì½˜ ì‚¬ìš©
        emojis = ["ğŸ˜Š", "ğŸ½ï¸", "âœ¨", "ğŸ‘", "ğŸ˜„"]
        if any(emoji in response for emoji in emojis):
            score += 0.1
        
        # ë¶€ì •ì  í‘œí˜„ ì œì™¸
        negative_words = ["ëª¨ë¥´ê² ", "ì£„ì†¡", "ì‹¤íŒ¨", "ì˜¤ë¥˜"]
        if any(word in response for word in negative_words):
            score -= 0.2
        
        return max(0.0, min(1.0, score))
    
    def _should_deploy_new_adapter(self, adapter_name: str, performance: Dict[str, float]) -> bool:
        """ìƒˆ ì–´ëŒ‘í„° ë°°í¬ ì—¬ë¶€ ê²°ì •"""
        new_score = performance.get("overall_score", 0.0)
        
        # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€
        if new_score < 0.6:
            return False
        
        # ê¸°ì¡´ ì–´ëŒ‘í„°ì™€ ë¹„êµ
        if not self.adapter_performance:
            return True  # ì²« ë²ˆì§¸ ì–´ëŒ‘í„°
        
        # ìµœê³  ì„±ëŠ¥ ì–´ëŒ‘í„°ì™€ ë¹„êµ
        best_score = max(
            perf.get("overall_score", 0.0) 
            for perf in self.adapter_performance.values()
        )
        
        # 5% ì´ìƒ ê°œì„ ë˜ì–´ì•¼ ë°°í¬
        improvement_threshold = 0.05
        return new_score > (best_score + improvement_threshold)
    
    def _deploy_adapter(self, adapter_name: str):
        """ì–´ëŒ‘í„° ë°°í¬ (í™œì„±í™”)"""
        try:
            adapter_path = self.active_adapters.get(adapter_name)
            if not adapter_path:
                raise ValueError(f"ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_name}")
            
            # í˜„ì¬ í™œì„± ì–´ëŒ‘í„°ë¡œ ì„¤ì •í•˜ëŠ” ë¡œì§
            # (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ë¡œë”© ì‹œìŠ¤í…œê³¼ ì—°ë™)
            
            logger.info(f"ì–´ëŒ‘í„° ë°°í¬ ì™„ë£Œ: {adapter_name}")
            
            # ë°°í¬ ê¸°ë¡
            deployment_record = {
                "adapter_name": adapter_name,
                "deployed_at": datetime.now().isoformat(),
                "performance": self.adapter_performance.get(adapter_name, {})
            }
            
            # ë°°í¬ ê¸°ë¡ ì €ì¥
            deployment_file = self.base_save_path / "deployments.jsonl"
            with open(deployment_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(deployment_record, ensure_ascii=False) + "\n")
                
        except Exception as e:
            logger.error(f"ì–´ëŒí„° ë°°í¬ ì‹¤íŒ¨: {e}")
            raise
    
    def get_training_status(self) -> Dict[str, Any]:
        """í›ˆë ¨ ìƒíƒœ ì¡°íšŒ"""
        return {
            "is_training": self.is_training,
            "last_training_time": self.last_training_time.isoformat() if self.last_training_time else None,
            "current_adapter_version": self.current_adapter_version,
            "active_adapters": list(self.active_adapters.keys()),
            "training_history_count": len(self.training_history),
            "auto_training_enabled": self.config.auto_training_enabled,
            "next_training_check": self._get_next_training_time().isoformat() if self.last_training_time else "soon"
        }
    
    def _get_next_training_time(self) -> datetime:
        """ë‹¤ìŒ í›ˆë ¨ ì˜ˆì • ì‹œê°„"""
        if not self.last_training_time:
            return datetime.now()
        
        return self.last_training_time + timedelta(hours=self.config.training_interval_hours)
    
    def get_adapter_performance_report(self) -> Dict[str, Any]:
        """ì–´ëŒ‘í„° ì„±ëŠ¥ ë¦¬í¬íŠ¸"""
        if not self.adapter_performance:
            return {"message": "í‰ê°€ëœ ì–´ëŒ‘í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ìµœê³  ì„±ëŠ¥ ì–´ëŒ‘í„°
        best_adapter = max(
            self.adapter_performance.items(),
            key=lambda x: x[1].get("overall_score", 0.0)
        )
        
        # í‰ê·  ì„±ëŠ¥
        avg_score = sum(
            perf.get("overall_score", 0.0) 
            for perf in self.adapter_performance.values()
        ) / len(self.adapter_performance)
        
        return {
            "total_adapters": len(self.adapter_performance),
            "best_adapter": {
                "name": best_adapter[0],
                "score": best_adapter[1].get("overall_score", 0.0)
            },
            "average_score": avg_score,
            "performance_history": [
                {
                    "adapter": name,
                    "score": perf.get("overall_score", 0.0),
                    "evaluated_at": perf.get("evaluation_timestamp")
                }
                for name, perf in self.adapter_performance.items()
            ]
        }
    
    def cleanup_old_adapters(self, keep_count: int = 5):
        """ì˜¤ë˜ëœ ì–´ëŒ‘í„° ì •ë¦¬"""
        if len(self.active_adapters) <= keep_count:
            return
        
        # ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ì„±ëŠ¥ ì–´ëŒ‘í„° ì œê±°)
        sorted_adapters = sorted(
            self.active_adapters.items(),
            key=lambda x: self.adapter_performance.get(x[0], {}).get("overall_score", 0.0),
            reverse=True
        )
        
        # ìƒìœ„ ì–´ëŒ‘í„°ë§Œ ìœ ì§€
        adapters_to_keep = dict(sorted_adapters[:keep_count])
        adapters_to_remove = dict(sorted_adapters[keep_count:])
        
        # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì œê±°
        for adapter_name, adapter_path in adapters_to_remove.items():
            try:
                import shutil
                if Path(adapter_path).exists():
                    shutil.rmtree(adapter_path)
                logger.info(f"ì–´ëŒ‘í„° ì œê±°: {adapter_name}")
            except Exception as e:
                logger.warning(f"ì–´ëŒ‘í„° ì œê±° ì‹¤íŒ¨ {adapter_name}: {e}")
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        self.active_adapters = adapters_to_keep
        
        for adapter_name in adapters_to_remove:
            if adapter_name in self.adapter_performance:
                del self.adapter_performance[adapter_name]
        
        logger.info(f"ì–´ëŒ‘í„° ì •ë¦¬ ì™„ë£Œ: {len(adapters_to_remove)}ê°œ ì œê±°, {len(adapters_to_keep)}ê°œ ìœ ì§€")

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_lora_trainer(model: KoAlpacaModel, data_collector: LearningDataCollector, 
                       config: LoRATrainingConfig = None) -> NaviyamLoRATrainer:
    """LoRA í›ˆë ¨ê¸° ìƒì„±"""
    if config is None:
        config = LoRATrainingConfig()
    
    return NaviyamLoRATrainer(model, config, data_collector)

def start_auto_lora_training(trainer: NaviyamLoRATrainer):
    """ìë™ LoRA í›ˆë ¨ ì‹œì‘ (í¸ì˜ í•¨ìˆ˜)"""
    trainer.start_auto_training()
    return trainer.get_training_status()